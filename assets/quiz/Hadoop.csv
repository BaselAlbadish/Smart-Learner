,Question,Answers,Right Index
0,Partitioner controls the partitioning of what data?,"['final keys', 'final values', 'intermediate keys', 'intermediate values']",2
1,SQL Windowing functions are implemented in Hive using which keywords?,"['UNION DISTINCT, RANK', 'OVER, RANK', 'OVER, EXCEPT', 'UNION DISTINCT, RANK']",1
2,"Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","['Add a partitioned shuffle to the Map job.', 'Add a partitioned shuffle to the Reduce job.', 'Break the Reduce job into multiple, chained Reduce jobs.', 'Break the Reduce job into multiple, chained Map jobs.']",1
3,"Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","['encrypted HTTP', 'unsigned HTTP', 'compressed HTTP', 'signed HTTP']",3
4,MapReduce jobs can be written in which language?,"['Java or Python', 'SQL only', 'SQL or Java', 'Python or SQL']",0
5,"To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","['Reducer', 'Combiner', 'Mapper', 'Counter']",1
6,"To verify job status, look for the value ___ in the ___.","['SUCCEEDED; syslog', 'SUCCEEDED; stdout', 'DONE; syslog', 'DONE; stdout']",1
7,Which line of code implements a Reducer method in MapReduce 2.0?,"['public void reduce(Text key, Iterator values, Context context){â€¦}', 'public static void reduce(Text key, IntWritable[] values, Context context){â€¦}', 'public static void reduce(Text key, Iterator values, Context context){â€¦}', 'public void reduce(Text key, IntWritable[] values, Context context){â€¦}']",0
8,"To get the total number of mapped input records in a map job task, you should review the value of which counter?","['FileInputFormatCounter', 'FileSystemCounter', 'JobCounter', 'TaskCounter (NOT SURE)']",3
9,Hadoop Core supports which CAP capabilities?,"['A, P', 'C, A', 'C, P', 'C, A, P']",0
10,What are the primary phases of a Reducer?,"['combine, map, and reduce', 'shuffle, sort, and reduce', 'reduce, sort, and combine', 'map, sort, and combine']",1
11,"To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the ___ service, which is ___.","['Oozie; open source', 'Oozie; commercial software', 'Zookeeper; commercial software', 'Zookeeper; open source']",3
12,"For high availability, use multiple nodes of which type?","['data', 'name', 'memory', 'worker']",1
13,DataNode supports which type of drives?,"['hot swappable', 'cold swappable', 'warm swappable', 'non-swappable']",0
14,Which method is used to implement Spark jobs?,"['on disk of all workers', 'on disk of the master node', 'in memory of the master node', 'in memory of all workers']",3
15,"In a MapReduce job, where does the map() function run?","['on the reducer nodes of the cluster', 'on the data nodes of the cluster (NOT SURE)', 'on the master node of the cluster', 'on every node of the cluster']",1
16,"To reference a master file for lookups during Mapping, what type of cache should be used?","['distributed cache', 'local cache', 'partitioned cache', 'cluster cache']",0
17,Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?,"['cache inputs', 'reducer inputs', 'intermediate values', 'map inputs']",3
18,Which command imports data to Hadoop from a MySQL database?,"['spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark', 'sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop', 'sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop', 'spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark']",2
19,In what form is Reducer output presented?,"['compressed (NOT SURE)', 'sorted', 'not sorted', 'encrypted']",0
20,Which library should be used to unit test MapReduce code?,"['JUnit', 'XUnit', 'MRUnit', 'HadoopUnit']",2
21,"If you started the NameNode, then which kind of user must you be?","['hadoop-user', 'super-user', 'node-user', 'admin-user']",1
22,State _ between the JVMs in a MapReduce job,"['can be configured to be shared', 'is partially shared', 'is shared', 'is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)']",3
23,"To create a MapReduce job, what should be coded first?","['a static job() method', 'a Job class and instance (NOT SURE)', 'a job() method', 'a static Job class']",1
24,"To connect Hadoop to AWS S3, which client should you use?","['S3A', 'S3N', 'S3', 'the EMR S3']",0
25,HBase works with which type of schema enforcement?,"['schema on write', 'no schema', 'external schema', 'schema on read']",3
26,HDFS file are of what type?,"['read-write', 'read-only', 'write-only', 'append-only']",3
27,A distributed cache file path can originate from what location?,"['hdfs or top', 'http', 'hdfs or http', 'hdfs']",2
28,Which library should you use to perform ETL-type MapReduce jobs?,"['Hive', 'Pig', 'Impala', 'Mahout']",1
29,What is the output of the Reducer?,"['a relational table', 'an update to the input file', 'a single, combined list', 'a set of  pairs']",3
30,"To optimize a Mapper, what should you perform first?","['Override the default Partitioner.', 'Skip bad records.', 'Break up Mappers that do more than one task into multiple Mappers.', 'Combine Mappers that do one task into large Mappers.']",-1
31,"When implemented on a public cloud, with what does Hadoop processing interact?","['files in object storage', 'graph data in graph databases', 'relational data in managed RDBMS systems', 'JSON data in NoSQL databases']",0
32,"In the Hadoop system, what administrative mode is used for maintenance?","['data mode', 'safe mode', 'single-user mode', 'pseudo-distributed mode']",1
33,In what format does RecordWriter write an output file?,"['pairs', 'keys', 'values', 'pairs']",0
34,To what does the Mapper map input key/value pairs?,"['an average of keys for values', 'a sum of keys for values', 'a set of intermediate key/value pairs', 'a set of final key/value pairs']",2
35,"Which Hive query returns the first 1,000 values?","['SELECTâ€¦WHERE value = 1000', 'SELECT â€¦ LIMIT 1000', 'SELECT TOP 1000 â€¦', 'SELECT MAX 1000â€¦']",1
36,"To implement high availability, how many instances of the master node should you configure?","['one', 'zero', 'shared', 'two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)']",3
37,Hadoop 2.x and later implement which service as the resource coordinator?,"['kubernetes', 'JobManager', 'JobTracker', 'YARN']",3
38,"In MapReduce, _ have _","['tasks; jobs', 'jobs; activities', 'jobs; tasks', 'activities; tasks']",2
39,What type of software is Hadoop Common?,"['database', 'distributed computing framework', 'operating system', 'productivity tool']",1
40,"If no reduction is desired, you should set the numbers of _ tasks to zero","['combiner', 'reduce', 'mapper', 'intermediate']",1
41,MapReduce applications use which of these classes to report their statistics?,"['mapper', 'reducer', 'combiner', 'counter']",3
42,"_ is the query language, and _ is storage for NoSQL on Hadoop","['HDFS; HQL', 'HQL; HBase', 'HDFS; SQL', 'SQL; HBase']",1
43,MapReduce 1.0 _ YARN,"['does not include', 'is the same thing as', 'includes', 'replaces']",0
44,"Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories?","['ControllerNode', 'DataNode', 'MetadataNode', 'NameNode']",3
45,HQL queries produce which job types?,"['Impala', 'MapReduce', 'Spark', 'Pig']",-1
46,Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line 2 below?,"['as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);', 'as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);', 'as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);', 'as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);']",1
47,"In a MapReduce job, which phase runs after the Map phase completes?","['Combiner', 'Reducer', 'Map2', 'Shuffle and Sort']",0
48,Where would you configure the size of a block in a Hadoop environment?,"['dfs.block.size in hdfs-site.xmls', 'orc.write.variable.length.blocks in hive-default.xml', 'mapreduce.job.ubertask.maxbytes in mapred-site.xml', 'hdfs.block.size in hdfs-site.xml']",0
49,Hadoop systems are _ RDBMS systems.,"['replacements for', 'not used with', 'substitutes for', 'additions for']",3
50,Which object can be used to distribute jars or libraries for use in MapReduce tasks?,"['distributed cache', 'library manager', 'lookup store', 'registry']",0
51,"To view the execution details of an Impala query plan, which function would you use ?","['explain', 'query action', 'detail', 'query plan']",0
52,Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time?,"['partitioning', 'snapshot', 'replication', 'high availability']",1
53,Hadoop Common is written in which language?,"['C++', 'C', 'Haskell', 'Java']",3
54,Which file system does Hadoop use for storage?,"['NAS', 'FAT', 'HDFS', 'NFS']",2
55,What kind of storage and processing does Hadoop support?,"['encrypted', 'verified', 'distributed', 'remote']",2
56,Hadoop Common consists of which components?,"['Spark and YARN', 'HDFS and MapReduce', 'HDFS and S3', 'Spark and MapReduce']",-1
57,Most Apache Hadoop committers' work is done at which commercial company?,"['Cloudera', 'Microsoft', 'Google', 'Amazon']",-1
58,"To get information about Reducer job runs, which object should be added?","['Reporter', 'IntReadable', 'IntWritable', 'Writer']",-1
59,"After changing the default block size and restarting the cluster, to which data does the new size apply?","['all data', 'no data', 'existing data', 'new data']",-1
60,Which statement should you add to improve the performance of the following query?,"['GROUP BY', 'FILTER', 'SUB-SELECT', 'SORT']",-1
61,What custom object should you implement to reduce IO in MapReduce?,"['Comparator', 'Mapper', 'Combiner', 'Reducer']",-1
62,You can optimize Hive queries using which method?,"['secondary indices', 'summary statistics', 'column-based statistics', 'a primary key index']",-1
63,"If you are processing a single action on each input, what type of job should you create?","['partition-only', 'map-only', 'reduce-only', 'combine-only']",-1
64,The simplest possible MapReduce job optimization is to perform which of these actions?,"['Add more master nodes.', 'Implement optimized InputSplits.', 'Add more DataNodes.', 'Implement a custom Mapper.']",-1
65,"When you implement a custom Writable, you must also define which of these object?","['a sort policy', 'a combiner policy', 'a compression policy', 'a filter policy']",-1
66,"To copy a file into the Hadoop file system, what command should you use?","['hadoop fs -copy', 'hadoop fs -copy', 'hadoop fs -copyFromLocal', 'hadoop fs -copyFromLocal']",2
